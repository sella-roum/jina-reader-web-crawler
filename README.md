# Jina Reader Web クローラー

## 概要

このアプリケーションは、指定された URL からウェブページのコンテンツを取得し、そのページ内のリンクを再帰的にクローリングすることができる Web アプリケーションです。Jina Reader API を利用してコンテンツを取得し、クローリング結果はブラウザの IndexedDB に保存されます。

ユーザーは初期 URL を入力して最初のページを取得し、そこから抽出されたリンクを選択してクローリングを実行できます。クローリングの進捗状況を確認したり、完了した結果を閲覧・管理したりすることが可能です。

## 主な機能

- **初期 URL からのコンテンツ取得**: 指定した URL のコンテンツを Jina Reader API 経由で取得します。
- **リンク抽出**: 取得したコンテンツ（Markdown 形式）から同じドメイン内のリンクを抽出します。
- **URL 選択**: 抽出されたリンクの中から、次にクローリングしたい URL を選択できます（複数選択、全選択/解除、検索フィルタリング可能）。
- **選択的クローリング**: 選択した URL のコンテンツを並列で取得します（並列数は設定可能）。
- **進捗表示**: クローリングの進捗状況、統計情報（合計、完了、処理中、エラー）、各 URL のステータスをリアルタイムで表示します。
- **エラーハンドリングとリトライ**: コンテンツ取得に失敗した URL を特定し、エラーとなった URL のみを再試行する機能を提供します。
- **結果の永続化**: クローリングが完了したページのデータを IndexedDB に保存します。
- **結果の閲覧・管理**:
  - 保存された結果を一覧表示します。
  - 検索（URL、コンテンツ）、ソート（URL 順、サイズ順）、ページネーション機能を提供します。
  - 個別の結果を詳細画面で Markdown または HTML 形式で表示できます。
  - 結果をダウンロードできます（個別または一覧、JSON, Markdown, Text, HTML 形式）。
  - 不要な結果を削除できます（個別または一覧）。
- **再クローリング**: 結果一覧画面から URL を複数選択し、再度クローリングを開始できます。
- **設定**: クローリング時の最大並列処理数を設定できます（設定は IndexedDB に保存）。
- **状態復元**: クローリング中の状態（入力 URL、抽出 URL リスト、進捗など）を Local Storage に保存し、ページリロード後も作業を継続できます。
- **レスポンシブデザイン**: モバイルデバイスにも対応しています。

## 使用技術

- **フレームワーク**: Next.js (v15.2.4, App Router)
- **言語**: TypeScript
- **UI ライブラリ**:
  - shadcn/ui (Radix UI + Tailwind CSS)
  - Tailwind CSS (v3.4.17)
  - Lucide React (アイコン)
- **状態管理**: React Hooks (`useState`, `useEffect`, `useCallback`, `useMemo`), Custom Hooks (`useCrawler`, `useToast`, `useIsMobile`)
- **データ永続化**:
  - IndexedDB (クローリング結果、設定の永続化)
  - Local Storage (クローリング中の状態の一時保存)
- **API**: Jina Reader API (`https://r.jina.ai/`)
- **アニメーション**: Framer Motion
- **リスト表示**: React Virtuoso (仮想スクロール)
- **Markdown 処理**: Marked
- **その他**: Sonner (Toast 通知), Vaul (Drawer)

## 画面構成

1.  **クローラー画面 (`/`)**:
    - アプリケーションのメイン画面。
    - URL 入力フォーム (`UrlInputForm`)。
    - 抽出された URL の選択リスト (`UrlSelectorList`)。
    - クローリング進捗表示エリア (`CrawlingProgress`)。
    - 結果一覧へのリンク、クローリング情報リセットボタン。
2.  **結果一覧画面 (`/results`)**:
    - IndexedDB に保存されたクローリング結果を一覧表示 (`ResultsList`)。
    - 検索、ソート、フィルタリング、一括ダウンロード/削除、再クローリング選択機能。
    - 個々の結果カード (`ResultItem`) を表示し、詳細画面へのリンクを提供。
3.  **結果詳細画面 (`/results/[url]`)**:
    - 特定の URL のクローリング結果を詳細表示。
    - Markdown/HTML 表示切り替え、ダウンロード、削除機能。

## セットアップと実行方法

### 必要なもの

- Node.js (v18.18.0 以上推奨)
- pnpm (パッケージマネージャー)

### 手順

1.  **リポジトリをクローン**:
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    ```
2.  **依存関係をインストール**:
    ```bash
    pnpm install
    ```
3.  **開発サーバーを起動**:

    ```bash
    pnpm dev
    ```

    ブラウザで `http://localhost:3000` を開きます。

4.  **(オプション) 本番ビルド**:
    ```bash
    pnpm build
    ```
5.  **(オプション) 本番サーバーを起動**:
    ```bash
    pnpm start
    ```

## ファイル構成概要

- **`/app`**: Next.js App Router の規約に基づいたルーティング、レイアウト、ページコンポーネント。
  - `/results`: 結果一覧・詳細画面関連。
- **`/components`**: 再利用可能な React コンポーネント。
  - `/ui`: shadcn/ui によって生成・カスタマイズされたコンポーネント。
  - その他: アプリケーション固有のコンポーネント (`UrlInputForm`, `UrlSelectorList`, `CrawlingProgress`, `ResultsList`, `ResultItem` など)。
- **`/hooks`**: カスタム React フック。
  - `use-crawler.ts`: クローリングの主要なロジックを管理。
  - `use-toast.ts`, `use-mobile.tsx`: UI 関連のフック。
- **`/lib`**: ユーティリティ関数やライブラリ関連のコード。
  - `indexed-db.ts`: IndexedDB とのインタラクションを管理。
  - `utils.ts`: `cn` 関数など。
  - `url-utils.ts`: URL のエンコード/デコード用。
- **`/public`**: アイコン、画像などの静的ファイル。
- **`/types`**: TypeScript の型定義 (`CrawledPage`, `ExtractedUrl`)。
- **`/utils`**: アプリケーション固有のユーティリティ関数。
  - `url-utils.ts`: Markdown からのリンク抽出、相対 URL 解決、ドメイン比較など。
- **設定ファイル**: `next.config.mjs`, `tailwind.config.ts`, `tsconfig.json` など。

## データ永続化

- **IndexedDB**:
  - クローリングが**完了**したページのデータ (`CrawledPage`) を永続的に保存します。これにより、ブラウザを閉じても結果が保持されます。
  - クローリング設定（最大並列処理数）もここに保存されます。
- **Local Storage**:
  - クローリング**中**の状態（初期 URL、現在のクローリングデータリスト、抽出 URL リスト、進捗、クローリング/リトライ状態）を一時的に保存します。これにより、ページをリロードしても途中から作業を再開できます。
  - 結果一覧画面からクローラー画面へ URL リストを渡す際にも一時的に使用されます。
  - クローリング情報リセット機能でクリアされます。

## 注意点・制限事項

- コンテンツの取得は Jina Reader API (`https://r.jina.ai/`) に依存しています。API の仕様変更、レート制限、サービス停止などの影響を受ける可能性があります。
- 一部のウェブサイト（JavaScript による動的レンダリングが必須なサイト、ログインが必要なサイトなど）では、Jina Reader API が正しくコンテンツを取得できない場合があります。
- クローリングはクライアントサイド（ブラウザ）で行われるため、非常に大量の URL を一度に処理すると、ブラウザのメモリや CPU リソースを消費し、パフォーマンスに影響が出る可能性があります。並列処理数の設定で調整してください。
- リンク抽出は、初期 URL と同じドメイン内のリンクのみを対象としています。外部ドメインへのリンクは抽出しません。
- Markdown および基本的な HTML の `<a>` タグからのリンク抽出に対応していますが、JavaScript で生成されるリンクなどは抽出できません。
